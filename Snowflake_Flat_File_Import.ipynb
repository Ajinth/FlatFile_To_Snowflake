{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this python notebook is to read the list of excel files from a specific folder and stage it into a snowflake schema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Anaconda installation of python - preferrably python 3\n",
    "\n",
    "2) Snowflake connector for python (pip install --upgrade snowflake-connector-python)\n",
    "\n",
    "3) Sqlalchemy connector for snowflake (pip install --upgrade snowflake-sqlalchemy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions to use this Notebook\n",
    "\n",
    "1) Update the credentials for snowflake environments in the global variable section. The following variables should be updated - user, password, acount, database,schema, warehouse and role  \n",
    "\n",
    "2) Update the \"skip_rows\" variable to indicate the number of rows to be skipped while reading into a dataframe\n",
    "\n",
    "3) Update the \"sheet\" variable to indicate the sheet name to be read into a dataframe\n",
    "\n",
    "4) Copy the flat files (excels) to the same location as this python notebook \n",
    "\n",
    "5) Update the Source_File excel with the list of files names (include extension) in column A. Provide the table name in column B \n",
    "\n",
    "6)  To execute the notebook click on \"Kernel\" and select \"Restart and Run all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Python Libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import pprint\n",
    "import xlrd\n",
    "import datetime \n",
    "\n",
    "#Python Libraries for Snowflake \n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Code to hide warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Global Variables\n",
    "\n",
    "'''Credentials for Snowflake Environments'''\n",
    "\n",
    "user='<Enter the username>'\n",
    "password='<Enter the password>'\n",
    "account='<Enter the Snowflake account>'\n",
    "database = '<Enter the database name>'\n",
    "schema = '<Enter the schema name>'\n",
    "warehouse = '<Enter the warehouse name>'\n",
    "role='<Enter the role name>'\n",
    "\n",
    "\n",
    "'''Flat File Details'''\n",
    "\n",
    "file ='<Enter the flat file name>'\n",
    "skip_rows=6\n",
    "sheet='Main_Data'\n",
    "batch_size = 5000\n",
    "\n",
    "'''Variables for Connection Initializer'''\n",
    "use_db = 'USE DATABASE ' + str(database)\n",
    "use_warehouse = 'USE WAREHOUSE ' + str(warehouse)\n",
    "use_schema = 'USE SCHEMA ' + str(schema)\n",
    "\n",
    "# Initializing the lists to capture log metrics \n",
    "source_file_list = [] \n",
    "source_count_list = []\n",
    "target_table_list =[]\n",
    "target_table_count_list = [] \n",
    "load_date_time_list = []\n",
    "zippped_list = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section for Generic Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the Snowflake Engine \n",
    "'''This function takes three inputs user, password and account and returns an engine object'''\n",
    "def engine_creator(user, password, account): \n",
    "    engine_input_string = 'snowflake://' + str(user) + ':' + str(password)+ '@' + str(account)\n",
    "    engine = create_engine(engine_input_string)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the Snowflake connection from the engine\n",
    "'''This function takes the engine object as input and creates a connection object'''\n",
    "def engine_connection(engine): \n",
    "    connection = engine.connect()\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to initialize the connection \n",
    "'''This function takes connection, database, warehouse and schema as input and initializes the connection'''\n",
    "def connection_setup(connection, use_db, use_warehouse, use_schema): \n",
    "    connection.execute(use_db)\n",
    "    connection.execute(use_warehouse)\n",
    "    connection.execute(use_schema)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to commit to snowflake \n",
    "def commit_sql(connection): \n",
    "    connection.execute('Commit')\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the Source Dictionary from the input parameter file \n",
    "'''This function takes a source file and creates a python dictionary '''\n",
    "def source_dictionary_generator(file): \n",
    "    workbook=xlrd.open_workbook(file)\n",
    "    sheet=workbook.sheet_by_index(0)\n",
    "    \n",
    "    col_a = sheet.col_values(0, 1)\n",
    "    col_b = sheet.col_values(1, 1)\n",
    "    \n",
    "    source_dictionary = {a : b for a, b in zip(col_a, col_b)}\n",
    "    \n",
    "    return source_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create the dataframe\n",
    "'''It takes a file name and the sheet name as arguments and returns a dataframe''' \n",
    "def create_df(file_name,sheet,skip_rows): \n",
    "    df = pd.read_excel(file_name, skiprows=skip_rows, sheet_name=sheet)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to bulk load into Snowflake \n",
    "\n",
    "def bulk_load(connection, source_dictionary,schema, sheet): \n",
    "    '''Loop to process one file at a time and load into snowflake'''\n",
    "    \n",
    "    for key, value in source_dictionary.items(): \n",
    "        \n",
    "        df = create_df(key,sheet,skip_rows)\n",
    "        \n",
    "        '''Added to convert all values in a dataframe in string. Added to handle the SFDC-SLGX reports'''\n",
    "        df = df.applymap(str)\n",
    "        df_length = len(df)\n",
    "        \n",
    "        connection_setup(connection, use_db, use_warehouse, use_schema)\n",
    "        \n",
    "        '''Truncate table if already exists'''\n",
    "        drop_sql = 'DROP TABLE IF EXISTS ' + '\"' + str(database) + '\"' + \".\" + '\"' + str(schema) + '\"' + \".\" + '\"' + str(value) + '\"'\n",
    "        print(drop_sql)\n",
    "        connection.execute(drop_sql)\n",
    "        \n",
    "        '''Batching the dataframe to process it in chunks of 5000 batch sizes'''\n",
    "        print(\"LOADING SNAPSHOT {} INTO SNOWFLAKE\".format(key))\n",
    "        for batch_start in range(0,df_length, 5000): \n",
    "            batch_end = batch_start + batch_size \n",
    "            if batch_end > df_length: \n",
    "                batch_remaining = df_length - batch_start\n",
    "                batch_end = batch_start + batch_remaining\n",
    "                df_part = df.iloc[batch_start:batch_end]\n",
    "                print (\"LOADING BATCHES {} to {} now\".format(batch_start, batch_end))\n",
    "                df_part.to_sql(value, con=connection, schema=schema, if_exists='append', index=False)\n",
    "                commit_sql(connection)\n",
    "                \n",
    "            else: \n",
    "                df_part = df.iloc[batch_start:batch_end]\n",
    "                print (\"LOADING BATCHES {} to {} now\".format(batch_start, batch_end))\n",
    "                df_part.to_sql(value, con=connection, schema=schema, if_exists='append', index=False)\n",
    "                commit_sql(connection)\n",
    "                   \n",
    "        print(\"LOADED SNAPSHOT {} INTO SNOWFLAKE\".format(key))\n",
    "        source_count = len(df)\n",
    "        target_count_sql = \"SELECT COUNT(*) FROM \" + '\"' + str(database) + '\"' + \".\" + '\"' + str(schema) + '\"' + \".\" + '\"' + str(value) + '\"'\n",
    "        target_count = connection.execute(target_count_sql).fetchone()[0]\n",
    "        load_date_time = datetime.datetime.today().strftime(\"%d-%B-%Y %H:%M:%S\")\n",
    "        \n",
    "        '''Creating lists to store source fle name, source count, target table name, target table count, and load date-time'''\n",
    "        source_file_list.append(key)\n",
    "        source_count_list.append(source_count)\n",
    "        target_table_list.append(value)\n",
    "        target_table_count_list.append(target_count)\n",
    "        load_date_time_list.append(load_date_time)\n",
    "        \n",
    "    '''Creating the dataframe to store the log information'''\n",
    "    zippped_list = list(zip(source_file_list, source_count_list, target_table_list, target_table_count_list, load_date_time_list))\n",
    "    df_log = pd.DataFrame(zippped_list, columns=['Source_File', 'Source_Count', 'Target_Table', 'Target_Count', 'Load_DateTime'])\n",
    "        \n",
    "\n",
    "    print (\"-----------------------------------------------------\")\n",
    "    print (\"LOAD INTO SNOWFLAKE COMPLETED, UPDATING LOG TABLE\")\n",
    "    print (\"-----------------------------------------------------\")\n",
    "    \n",
    "    return df_log\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to add logging \n",
    "\n",
    "def logger(connection,o_bulk_load):\n",
    "    o_bulk_load.to_sql('IMPORT_LOG', con=connection, schema=schema, if_exists='append', index=False)\n",
    "    commit_sql(connection)\n",
    "    \n",
    "    print (\"-----------------------------------------------------\")\n",
    "    print (\"LOG TABLE UPDATED, PROCESS COMPLETE. PLEASE VALIDATE SNOWFLAKE\")\n",
    "    print (\"-----------------------------------------------------\")\n",
    "    \n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section to execute Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    engine = engine_creator(user, password, account)\n",
    "    connection = engine_connection(engine)\n",
    "    source_dictionary = source_dictionary_generator(file)\n",
    "    o_bulk_load = bulk_load(connection, source_dictionary, schema, sheet)\n",
    "    o_logger = logger(connection, o_bulk_load)\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__== \"__main__\":\n",
    "  main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
